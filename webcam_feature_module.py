import threading, queue, time, cv2
import face_recognition
from pipeline_data import PipelineData
from dataclasses import dataclass, field
from typing import Optional, List, Tuple
import numpy as np


class WebcamModule:
    """
    ì‘ì„±: ê¹€í‘¸ë¥¸ë“¤
    ì´ì¤‘ ìŠ¤ë ˆë“œ ì›¹ìº  & íŠ¹ì§• ì¶”ì¶œ ëª¨ë“ˆì´ë‹¤.
    ë©”ì¸ ìŠ¤ë ˆë“œ: ê³ ì† í”„ë ˆì„ ìº¡ì³
    ì›Œì»¤ ìŠ¤ë ˆë“œ: ML ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë ˆì„ ì „ì†¡
    """

    def __init__(self, camera_id=0, capture_fps=60, processing_fps=6, tracking_fps=60,
                 bbox_reduce_ratio=0.1, auto_brightness=False):
        """
        ì›¹ìº  ëª¨ë“ˆ ì´ˆê¸°í™”.

        :param camera_id: ì¹´ë©”ë¼ ì¥ì¹˜ ID (ê¸°ë³¸ê°’: 0)
        :param capture_fps: ë””ìŠ¤í”Œë ˆì´ ìŠ¤ë ˆë“œ ëª©í‘œ FPS (ê¸°ë³¸ê°’: 60)
        :param processing_fps: ML ì²˜ë¦¬ í”„ë ˆì„ ì „ì†¡ FPS (ê¸°ë³¸ê°’: 6)
        :param tracking_fps: ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì  FPS (ê¸°ë³¸ê°’: 60)
        :param bbox_reduce_ratio: ë°”ìš´ë”© ë°•ìŠ¤ ì¶•ì†Œ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1, 10% ì¶•ì†Œ)
        :param auto_brightness: ìë™ ë°ê¸° ì¡°ì • í™œì„±í™” (ê¸°ë³¸ê°’: False)
        """
        # ì¹´ë©”ë¼ ì„¤ì •
        self.cap = cv2.VideoCapture(camera_id)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        if not self.cap.isOpened():
            print(f"ì˜¤ë¥˜: ì¹´ë©”ë¼ ID {camera_id}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ì¹´ë©”ë¼ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€, ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš© ì¤‘ì´ì§€ ì•Šì€ì§€ í™•ì¸í•˜ì„¸ìš”.")
            raise cv2.error(f"ì¹´ë©”ë¼ {camera_id}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ëª©í‘œ FPS ì„¤ì •
        self.capture_fps = capture_fps
        self.processing_fps = processing_fps
        self.tracking_fps = tracking_fps

        # [â–³]ì‹¤í—˜ì‹¤ ê¸°ëŠ¥ ì„¤ì •
        self.bbox_reduce_ratio = bbox_reduce_ratio  # ë°”ìš´ë”© ë°•ìŠ¤ ì¶•ì†Œ ë¹„ìœ¨
        self.auto_brightness = auto_brightness  # ìë™ ë°ê¸° ì¡°ì • í™œì„±í™”
        self.brightness_threshold = 80  # ë°ê¸° ì„ê³„ê°’ (0~255, 80 ë¯¸ë§Œì´ë©´ ì–´ë‘ìš´ ê²ƒìœ¼ë¡œ íŒë‹¨)

        # ìŠ¤ë ˆë“œ ê°„ ê³µìœ  í”„ë ˆì„ (ìµœì‹  í”„ë ˆì„ë§Œ ìœ ì§€)
        self.latest_frame = None
        self.frame_lock = threading.Lock()

        # í”ŒëŸ¬í„° ì•±ìœ¼ë¡œ ì „ì†¡í•  í (JPEG ì¸ì½”ë”© í”„ë ˆì„)
        self.frame_queue = queue.Queue(maxsize=2)

        # ML íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì „ì†¡í•  í
        self.output_queue = None

        # ìŠ¤ë ˆë“œ ì œì–´ ë° í”Œë˜ê·¸
        self.running = False
        self.capture_thread = None
        self.processing_thread = None
        self.tracker = None

        # ë™ì‘ ìƒíƒœ ê´€ë¦¬
        self.processing_state = "RECOGNIZING"
        self.tracker = None
        self.state_lock = threading.Lock()
        self.pending_bbox_to_track = None

        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í”Œë˜ê·¸
        self.test_mode = False

        # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì  ì„¤ì •
        self.tracking_frame_count = 0
        self.redetection_interval = 10
        self.last_known_bbox = None
        self.tracked_face_encoding = None
        self.face_match_threshold = 0.6

    def get_latest_frame_jpeg(self):
        try:
            return self.frame_queue.get_nowait()
        except queue.Empty:
            return None

    def set_output_queue(self, output_queue):
        self.output_queue = output_queue

    def start(self):
        self.running = True

        self.capture_thread = threading.Thread(
            target=self._capture_and_encode_loop,
            name="WebcamCapture",
            daemon=True
        )
        self.capture_thread.start()

        self.processing_thread = threading.Thread(
            target=self._processing_loop,
            name="WebcamProcessing",
            daemon=True
        )
        self.processing_thread.start()

    def stop(self):
        self.running = False

        if self.capture_thread:
            self.capture_thread.join(timeout=1.0)
        if self.processing_thread:
            self.processing_thread.join(timeout=1.0)

        self.cap.release()

    def start_tracking_request(self, bbox: Tuple[int, int, int, int]):
        with self.state_lock:
            print(f"WebcamModule: ì¶”ì  ì‹œì‘ ìš”ì²­ ìˆ˜ì‹  (BBox: {bbox})")
            self.pending_bbox_to_track = bbox
            self.processing_state = "START_TRACKING"

    def stop_tracking_request(self):
        with self.state_lock:
            print("WebcamModule: ì¶”ì  ì¤‘ì§€ ìš”ì²­ ìˆ˜ì‹ ")
            self.processing_state = "RECOGNIZING"
            self.tracker = None
            self.pending_bbox_to_track = None
            self.tracking_frame_count = 0
            self.last_known_bbox = None
            self.tracked_face_encoding = None

    def enable_test_mode(self, enabled=True):
        print(f"WebcamModule: í…ŒìŠ¤íŠ¸ ëª¨ë“œ {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
        self.test_mode = enabled

    def set_auto_brightness(self, enabled=True):
        """
        ìë™ ë°ê¸° ì¡°ì • í™œì„±í™”/ë¹„í™œì„±í™”
        :param enabled: Trueì´ë©´ í™œì„±í™”, Falseì´ë©´ ë¹„í™œì„±í™”
        """
        self.auto_brightness = enabled
        print(f"WebcamModule: ìë™ ë°ê¸° ì¡°ì • {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")

    def _reduce_bbox(self, bbox: Tuple[int, int, int, int], ratio: float) -> Tuple[int, int, int, int]:
        """
        ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì¤‘ì‹¬ì„ ìœ ì§€í•˜ë©´ì„œ ì¶•ì†Œí•©ë‹ˆë‹¤.

        :param bbox: (x, y, w, h) í˜•íƒœì˜ ë°”ìš´ë”© ë°•ìŠ¤
        :param ratio: ì¶•ì†Œ ë¹„ìœ¨ (0.2 = 20% ì¶•ì†Œ)
        :return: ì¶•ì†Œëœ (x, y, w, h) ë°”ìš´ë”© ë°•ìŠ¤
        """
        x, y, w, h = bbox

        # ì¶•ì†Œí•  í¬ê¸° ê³„ì‚°
        reduce_w = int(w * ratio / 2)  # ì–‘ìª½ì—ì„œ ì¤„ì¼ ë„ˆë¹„
        reduce_h = int(h * ratio / 2)  # ìœ„ì•„ë˜ì—ì„œ ì¤„ì¼ ë†’ì´

        # ìƒˆë¡œìš´ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (ì¤‘ì‹¬ ìœ ì§€)
        new_x = x + reduce_w
        new_y = y + reduce_h
        new_w = w - (reduce_w * 2)
        new_h = h - (reduce_h * 2)

        # ìµœì†Œ í¬ê¸° ë³´ì¥ (ë„ˆë¬´ ì‘ì•„ì§€ì§€ ì•Šë„ë¡)
        new_w = max(new_w, 20)
        new_h = max(new_h, 20)

        return (new_x, new_y, new_w, new_h)

    def _adjust_brightness(self, frame: np.ndarray) -> Tuple[np.ndarray, bool]:
        """
        í”„ë ˆì„ì˜ ë°ê¸°ë¥¼ ë¶„ì„í•˜ê³  í•„ìš”ì‹œ ì¡°ì •í•©ë‹ˆë‹¤.

        :param frame: ì…ë ¥ í”„ë ˆì„ (BGR)
        :return: (ì¡°ì •ëœ í”„ë ˆì„, ì¡°ì • ì—¬ë¶€)
        """
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜í•˜ì—¬ í‰ê·  ë°ê¸° ê³„ì‚°
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        avg_brightness = np.mean(gray)

        # ì–´ë‘ìš´ ê²½ìš°ì—ë§Œ ì¡°ì •
        if avg_brightness < self.brightness_threshold:
            # CLAHE (Contrast Limited Adaptive Histogram Equalization) ì ìš©
            # ì§€ì—­ì ìœ¼ë¡œ íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”ë¥¼ ìˆ˜í–‰í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë°ê¸° ì¡°ì •
            lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
            l, a, b = cv2.split(lab)

            # CLAHE ì ìš© (ë°ê¸° ì±„ë„ì—ë§Œ)
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
            l = clahe.apply(l)

            # ì±„ë„ ë³‘í•© ë° BGRë¡œ ë³€í™˜
            adjusted_lab = cv2.merge([l, a, b])
            adjusted_frame = cv2.cvtColor(adjusted_lab, cv2.COLOR_LAB2BGR)

            return adjusted_frame, True

        return frame, False

    def _capture_and_encode_loop(self):
        print("WebcamModule._capture_and_encode_loop(): Capture Thread ì‹œì‘")

        frame_interval = 1.0 / self.capture_fps

        while self.running:
            start_time = time.time()

            ret, frame = self.cap.read()
            if not ret:
                time.sleep(0.1)
                continue

            # ìë™ ë°ê¸° ì¡°ì • ì ìš© (í™œì„±í™”ëœ ê²½ìš°)
            if self.auto_brightness:
                frame, was_adjusted = self._adjust_brightness(frame)

            with self.frame_lock:
                self.latest_frame = frame.copy()

            _, jpeg_buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 80])
            jpeg_bytes = jpeg_buffer.tobytes()

            try:
                self.frame_queue.put(jpeg_bytes, block=False)
            except queue.Full:
                try:
                    self.frame_queue.get_nowait()
                    self.frame_queue.put(jpeg_bytes, block=False)
                except (queue.Empty, queue.Full):
                    pass

            elapsed = time.time() - start_time
            sleep_time = frame_interval - elapsed
            if sleep_time > 0:
                time.sleep(sleep_time)

        print("WebcamModule._capture_and_encode_loop(): Capture Thread ì¢…ë£Œ")

    def _processing_loop(self):
        print("WebcamModule._processing_loop(): Processing Thread ì‹œì‘ (Background)")

        frame_interval = 1.0 / self.processing_fps

        while self.running:
            start_time = time.time()

            frame_to_process = None

            with self.frame_lock:
                if self.latest_frame is None:
                    time.sleep(0.01)
                    continue
                frame_to_process = self.latest_frame.copy()

            if frame_to_process is None:
                time.sleep(0.01)
                continue

            with self.state_lock:
                current_state = self.processing_state

            pipeline_data = PipelineData(frame=frame_to_process)

            if current_state == "START_TRACKING":
                with self.state_lock:
                    bbox = self.pending_bbox_to_track
                    self.pending_bbox_to_track = None

                if bbox is not None:
                    try:
                        x, y, w, h = bbox
                        x, y, w, h = int(x), int(y), int(w), int(h)
                        original_bbox = (x, y, w, h)

                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¶•ì†Œ ì ìš©
                        reduced_bbox = self._reduce_bbox(original_bbox, self.bbox_reduce_ratio)
                        x, y, w, h = reduced_bbox

                        print(f"  â†’ BBox ì¶•ì†Œ: {original_bbox} â†’ {reduced_bbox} ({self.bbox_reduce_ratio * 100:.0f}% ì¶•ì†Œ)")

                        # ì–¼êµ´ ì„ë² ë”© ì €ì¥ (ì›ë³¸ bbox ì‚¬ìš©)
                        rgb_frame = cv2.cvtColor(frame_to_process, cv2.COLOR_BGR2RGB)
                        orig_x, orig_y, orig_w, orig_h = original_bbox
                        face_encodings = face_recognition.face_encodings(
                            rgb_frame,
                            [(orig_y, orig_x + orig_w, orig_y + orig_h, orig_x)]
                        )

                        if face_encodings:
                            self.tracked_face_encoding = face_encodings[0]
                            print(f"  â†’ ì¶”ì  ëŒ€ìƒ ì–¼êµ´ ì„ë² ë”© ì €ì¥ ì™„ë£Œ (ë²¡í„° í¬ê¸°: {len(self.tracked_face_encoding)})")
                        else:
                            print("  âš ï¸ ê²½ê³ : ì–¼êµ´ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨ (ì¶”ì ì€ ê³„ì†ë¨)")
                            self.tracked_face_encoding = None

                        # ì¶•ì†Œëœ bboxë¡œ íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
                        self.tracker = cv2.legacy.TrackerCSRT_create()
                        success = self.tracker.init(frame_to_process, (x, y, w, h))

                        if success:
                            with self.state_lock:
                                self.processing_state = "TRACKING"
                            print(f"WebcamModule: ì¶”ì  ì‹œì‘ ì„±ê³µ (TRACKING ëª¨ë“œ ì§„ì…) - ì¶•ì†Œëœ BBox: ({x},{y},{w},{h})")
                            self.last_known_bbox = reduced_bbox
                            self.tracking_frame_count = 0
                            pipeline_data.bbox_coords = [reduced_bbox]
                        else:
                            print("WebcamModule: íŠ¸ë˜ì»¤ ì´ˆê¸°í™” ì‹¤íŒ¨ (init ë°˜í™˜ê°’ False)")
                            with self.state_lock:
                                self.processing_state = "RECOGNIZING"
                                self.tracker = None

                    except Exception as e:
                        print(f"WebcamModule: íŠ¸ë˜ì»¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                        print(f"  - BBox ê°’: {bbox}")
                        print(f"  - Frame shape: {frame_to_process.shape}")
                        with self.state_lock:
                            self.processing_state = "RECOGNIZING"
                            self.tracker = None
                else:
                    with self.state_lock:
                        self.processing_state = "RECOGNIZING"

            elif current_state == "TRACKING":
                self.tracking_frame_count += 1

                if self.tracking_frame_count % self.redetection_interval == 0:
                    print(f"\n{'=' * 60}")
                    print(f"WebcamModule: ì£¼ê¸°ì  ì¬ê°ì§€ ìˆ˜í–‰ (í”„ë ˆì„ {self.tracking_frame_count})")
                    print(f"{'=' * 60}")

                    rgb_frame = cv2.cvtColor(frame_to_process, cv2.COLOR_BGR2RGB)
                    face_locations = face_recognition.face_locations(rgb_frame)

                    if face_locations:
                        print(f"  â†’ {len(face_locations)}ê°œì˜ ì–¼êµ´ ê°ì§€ë¨")

                        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)

                        best_match = None
                        best_encoding = None
                        min_distance = float('inf')
                        best_face_distance = None
                        best_iou = 0.0

                        for idx, ((top, right, bottom, left), encoding) in enumerate(
                                zip(face_locations, face_encodings)):
                            x, y = int(left), int(top)
                            w, h = int(right - left), int(bottom - top)
                            detected_bbox = (x, y, w, h)

                            # IoU ê³„ì‚°
                            iou = 0.0
                            if self.last_known_bbox:
                                # ì¶•ì†Œëœ bboxì™€ ë¹„êµí•˜ë¯€ë¡œ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›í•˜ì—¬ ë¹„êµ
                                expanded_last_bbox = self._expand_bbox(self.last_known_bbox, self.bbox_reduce_ratio)
                                iou = self._calculate_iou(expanded_last_bbox, detected_bbox)

                            # ì–¼êµ´ ì„ë² ë”© ê±°ë¦¬ ê³„ì‚°
                            face_distance = None
                            if self.tracked_face_encoding is not None:
                                face_distance = face_recognition.face_distance([self.tracked_face_encoding], encoding)[
                                    0]

                            # ë³µí•© ì ìˆ˜ ê³„ì‚°
                            if face_distance is not None:
                                face_similarity = 1.0 - face_distance
                                combined_score = (iou * 0.4) + (face_similarity * 0.6)
                                distance = 1 - combined_score
                            else:
                                distance = 1 - iou

                            print(f"  ì–¼êµ´ #{idx + 1}: BBox={detected_bbox}")
                            print(f"    - IoU: {iou:.3f}")
                            if face_distance is not None:
                                print(f"    - ì–¼êµ´ ê±°ë¦¬: {face_distance:.3f} (ì„ê³„ê°’: {self.face_match_threshold})")
                                print(
                                    f"    - ë§¤ì¹­ ì—¬ë¶€: {'âœ“ ê°™ì€ ì‚¬ëŒ' if face_distance < self.face_match_threshold else 'âœ— ë‹¤ë¥¸ ì‚¬ëŒ'}")
                            print(f"    - ì¢…í•© ì ìˆ˜: {(1 - distance):.3f}")

                            if distance < min_distance:
                                min_distance = distance
                                best_match = detected_bbox
                                best_encoding = encoding
                                best_face_distance = face_distance
                                best_iou = iou

                        # ìµœì¢… íŒë‹¨
                        is_same_person = True
                        rejection_reason = None

                        if best_iou < 0.1:
                            is_same_person = False
                            rejection_reason = f"ìœ„ì¹˜ ë¶ˆì¼ì¹˜ (IoU: {best_iou:.3f} < 0.1)"
                        elif best_face_distance is not None and best_face_distance > self.face_match_threshold:
                            is_same_person = False
                            rejection_reason = f"ì–¼êµ´ ë¶ˆì¼ì¹˜ (ê±°ë¦¬: {best_face_distance:.3f} > {self.face_match_threshold})"

                        print(f"\n  [ìµœì¢… íŒì •]")
                        if is_same_person and best_match:
                            print(f"  âœ… ë™ì¼ ì¸ë¬¼ í™•ì¸")
                            print(f"    - ê°ì§€ëœ BBox (ì›ë³¸): {best_match}")

                            # ê°ì§€ëœ bboxë¥¼ ì¶•ì†Œí•˜ì—¬ íŠ¸ë˜ì»¤ì— ì „ë‹¬
                            reduced_best_match = self._reduce_bbox(best_match, self.bbox_reduce_ratio)
                            print(f"    - ì¶•ì†Œëœ BBox: {reduced_best_match}")
                            print(f"    - IoU: {best_iou:.3f}")

                            if best_face_distance is not None:
                                print(f"    - ì–¼êµ´ ê±°ë¦¬: {best_face_distance:.3f}")
                            print(f"  â†’ íŠ¸ë˜ì»¤ ì¬ì´ˆê¸°í™” ì§„í–‰...")

                            try:
                                x, y, w, h = reduced_best_match
                                self.tracker = cv2.legacy.TrackerCSRT_create()
                                success = self.tracker.init(frame_to_process, (x, y, w, h))

                                if success:
                                    old_bbox = self.last_known_bbox
                                    self.last_known_bbox = reduced_best_match
                                    self.tracked_face_encoding = best_encoding
                                    pipeline_data.bbox_coords = [reduced_best_match]
                                    print(f"  âœ“ íŠ¸ë˜ì»¤ ì¬ì´ˆê¸°í™” ì„±ê³µ")

                                    # í¬ê¸° ë³€í™” í‘œì‹œ
                                    if old_bbox:
                                        old_w, old_h = old_bbox[2], old_bbox[3]
                                        size_change = ((w - old_w) / old_w * 100, (h - old_h) / old_h * 100)
                                        print(f"  ğŸ“ í¬ê¸° ë³€í™”: ë„ˆë¹„ {size_change[0]:+.1f}%, ë†’ì´ {size_change[1]:+.1f}%")
                                else:
                                    print("  âœ— íŠ¸ë˜ì»¤ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ì¡´ ì¶”ì  ìœ ì§€")
                                    success, bbox = self.tracker.update(frame_to_process)
                                    if success and bbox is not None:
                                        x, y, w, h = bbox
                                        bbox_int = (int(x), int(y), int(w), int(h))
                                        self.last_known_bbox = bbox_int
                                        pipeline_data.bbox_coords = [bbox_int]
                            except Exception as e:
                                print(f"  âœ— ì¬ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
                                with self.state_lock:
                                    self.processing_state = "RECOGNIZING"
                                    self.tracker = None
                                    self.tracking_frame_count = 0
                        else:
                            print(f"  âŒ ë‹¤ë¥¸ ì¸ë¬¼ë¡œ íŒë‹¨: {rejection_reason}")
                            print(f"  â†’ ì¸ì‹ ëª¨ë“œë¡œ ë³µê·€")
                            with self.state_lock:
                                self.processing_state = "RECOGNIZING"
                                self.tracker = None
                                self.tracking_frame_count = 0
                                self.tracked_face_encoding = None
                    else:
                        print("  âœ— ì–¼êµ´ ë¯¸ê°ì§€")
                        print("  â†’ ì¸ì‹ ëª¨ë“œë¡œ ë³µê·€")
                        with self.state_lock:
                            self.processing_state = "RECOGNIZING"
                            self.tracker = None
                            self.tracking_frame_count = 0
                            self.tracked_face_encoding = None

                    print(f"{'=' * 60}\n")
                else:
                    # ì¼ë°˜ ì¶”ì  ìˆ˜í–‰
                    success, bbox = self.tracker.update(frame_to_process)
                    if success and bbox is not None:
                        x, y, w, h = bbox
                        bbox_int = (int(x), int(y), int(w), int(h))
                        self.last_known_bbox = bbox_int
                        pipeline_data.bbox_coords = [bbox_int]
                    else:
                        print("WebcamModule: ì¶”ì  ì‹¤íŒ¨. (RECOGNIZING ëª¨ë“œ ë³µê·€)")
                        with self.state_lock:
                            self.processing_state = "RECOGNIZING"
                            self.tracker = None
                            self.tracking_frame_count = 0

            elif current_state == "RECOGNIZING":
                rgb_frame = cv2.cvtColor(frame_to_process, cv2.COLOR_BGR2RGB)

                face_locations_dlib = face_recognition.face_locations(rgb_frame)
                face_encodings = face_recognition.face_encodings(rgb_frame, face_locations_dlib)

                bboxes_cv2 = []
                largest_bbox_area = -1
                largest_bbox = None

                for (top, right, bottom, left), encoding in zip(face_locations_dlib, face_encodings):
                    x, y = int(left), int(top)
                    w, h = int(right - left), int(bottom - top)
                    cv2_bbox = (x, y, w, h)

                    bboxes_cv2.append(cv2_bbox)
                    pipeline_data.face_vectors.append(encoding.tolist())

                    area = w * h
                    if area > largest_bbox_area:
                        largest_bbox_area = area
                        largest_bbox = cv2_bbox

                pipeline_data.bbox_coords = bboxes_cv2

                if self.test_mode and largest_bbox:
                    print(f"WebcamModule (Test Mode): ê°€ì¥ í° ì–¼êµ´ ê°ì§€, ì¶”ì  ì‹œì‘")
                    self.start_tracking_request(largest_bbox)

            if self.output_queue is not None:
                try:
                    self.output_queue.put(pipeline_data, block=False)
                except queue.Full:
                    try:
                        self.output_queue.get_nowait()
                        self.output_queue.put(pipeline_data, block=False)
                    except (queue.Empty, queue.Full):
                        pass

            elapsed = time.time() - start_time
            sleep_time = frame_interval - elapsed
            if sleep_time > 0:
                time.sleep(sleep_time)

        print("WebcamModule._processing_loop(): Processing Thread ì¢…ë£Œ")

    def _expand_bbox(self, bbox: Tuple[int, int, int, int], ratio: float) -> Tuple[int, int, int, int]:
        """
        ì¶•ì†Œëœ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›í•©ë‹ˆë‹¤.

        :param bbox: (x, y, w, h) í˜•íƒœì˜ ì¶•ì†Œëœ ë°”ìš´ë”© ë°•ìŠ¤
        :param ratio: ì›ë˜ ì¶•ì†Œì— ì‚¬ìš©ëœ ë¹„ìœ¨
        :return: í™•ì¥ëœ (x, y, w, h) ë°”ìš´ë”© ë°•ìŠ¤
        """
        x, y, w, h = bbox

        # í™•ì¥í•  í¬ê¸° ê³„ì‚° (ì¶•ì†Œì˜ ì—­ì—°ì‚°)
        expand_w = int(w * ratio / (2 * (1 - ratio)))
        expand_h = int(h * ratio / (2 * (1 - ratio)))

        # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        orig_x = x - expand_w
        orig_y = y - expand_h
        orig_w = w + (expand_w * 2)
        orig_h = h + (expand_h * 2)

        return (orig_x, orig_y, orig_w, orig_h)

    def _calculate_iou(self, bbox1, bbox2):
        """
        ë‘ ë°”ìš´ë”© ë°•ìŠ¤ ê°„ì˜ IoU (Intersection over Union) ê³„ì‚°
        :param bbox1: (x, y, w, h)
        :param bbox2: (x, y, w, h)
        :return: IoU ê°’ (0~1)
        """
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2

        # êµì§‘í•© ì˜ì—­ ê³„ì‚°
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)

        if x_right < x_left or y_bottom < y_top:
            return 0.0

        intersection_area = (x_right - x_left) * (y_bottom - y_top)

        # í•©ì§‘í•© ì˜ì—­ ê³„ì‚°
        bbox1_area = w1 * h1
        bbox2_area = w2 * h2
        union_area = bbox1_area + bbox2_area - intersection_area

        return intersection_area / union_area if union_area > 0 else 0.0

